{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12351525,"sourceType":"datasetVersion","datasetId":7786924}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import zipfile\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom tensorflow.keras import layers, regularizers\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import img_to_array, array_to_img\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:38:05.322469Z","iopub.execute_input":"2025-07-02T12:38:05.323116Z","iopub.status.idle":"2025-07-02T12:38:19.638267Z","shell.execute_reply.started":"2025-07-02T12:38:05.323092Z","shell.execute_reply":"2025-07-02T12:38:19.637715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dir = \"/kaggle/input/teeth-cellula/Teeth_Dataset/Training\"\nval_dir   = \"/kaggle/input/teeth-cellula/Teeth_Dataset/Validation\"\ntest_dir  = \"/kaggle/input/teeth-cellula/Teeth_Dataset/Testing\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:41:56.581889Z","iopub.execute_input":"2025-07-02T12:41:56.582517Z","iopub.status.idle":"2025-07-02T12:41:56.586280Z","shell.execute_reply.started":"2025-07-02T12:41:56.582487Z","shell.execute_reply":"2025-07-02T12:41:56.585440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = os.listdir(train_dir)\nclass_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in classes}\nclass_names = list(class_counts.keys())\nclass_values = list(class_counts.values())\nplt.figure(figsize=(10,6))\nsns.barplot(x=class_names, y=class_values, palette='viridis')\nplt.title(\"Training Set Class Distribution\")\nplt.ylabel(\"Number of Images\")\nplt.xlabel(\"Class Name\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:42:14.142325Z","iopub.execute_input":"2025-07-02T12:42:14.142627Z","iopub.status.idle":"2025-07-02T12:42:14.496228Z","shell.execute_reply.started":"2025-07-02T12:42:14.142605Z","shell.execute_reply":"2025-07-02T12:42:14.495458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nfor idx, class_name in enumerate(class_names[:7]):\n    class_folder = os.path.join(train_dir, class_name)\n    image_path = os.path.join(class_folder, random.choice(os.listdir(class_folder)))\n    img = load_img(image_path, target_size=(224, 224))\n\n    plt.subplot(2, 4, idx + 1)\n    plt.imshow(img)\n    plt.title(class_name)\n    plt.axis('off')\n\nplt.suptitle(\"Sample Images from Each Class\", fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:42:52.186873Z","iopub.execute_input":"2025-07-02T12:42:52.187140Z","iopub.status.idle":"2025-07-02T12:42:53.139185Z","shell.execute_reply.started":"2025-07-02T12:42:52.187120Z","shell.execute_reply":"2025-07-02T12:42:53.138451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_datagen= ImageDataGenerator(\n    rescale=1./255,\n    zoom_range=0.1,\n    horizontal_flip=True\n)\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:43:07.890768Z","iopub.execute_input":"2025-07-02T12:43:07.891456Z","iopub.status.idle":"2025-07-02T12:43:07.895129Z","shell.execute_reply.started":"2025-07-02T12:43:07.891430Z","shell.execute_reply":"2025-07-02T12:43:07.894359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_class = class_names[3]\nsample_image_path = os.path.join(train_dir, sample_class, os.listdir(os.path.join(train_dir, sample_class))[0])\n\n# Load image and convert it to array\nimg = load_img(sample_image_path, target_size=(224, 224))\nx = img_to_array(img)\nx = x.reshape((1,) + x.shape)  # Reshape to match input shape expected by flow()\n\n# Plot original image\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 6, 1)\nplt.imshow(img)\nplt.title(\"Original\")\nplt.axis('off')\n\n# Generate and plot 5 augmented images\ni = 2\nfor batch in train_datagen.flow(x, batch_size=1):\n    plt.subplot(1, 6, i)\n    aug_img = array_to_img(batch[0])\n    plt.imshow(aug_img)\n    plt.title(\"Augmented\")\n    plt.axis('off')\n    i += 1\n    if i > 3:\n        break\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:43:35.613124Z","iopub.execute_input":"2025-07-02T12:43:35.613372Z","iopub.status.idle":"2025-07-02T12:43:35.852463Z","shell.execute_reply.started":"2025-07-02T12:43:35.613355Z","shell.execute_reply":"2025-07-02T12:43:35.851741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n    train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:43:54.493951Z","iopub.execute_input":"2025-07-02T12:43:54.494698Z","iopub.status.idle":"2025-07-02T12:43:56.373648Z","shell.execute_reply.started":"2025-07-02T12:43:54.494672Z","shell.execute_reply":"2025-07-02T12:43:56.372875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model= tf.keras.Sequential([\n    layers.Conv2D(256, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.0001), input_shape=(224, 224, 3)),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.BatchNormalization(),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.BatchNormalization(),\n\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.BatchNormalization(),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(7, activation='softmax')\n])\nmodel.compile(optimizer='adam', loss='categorical_crossentropy' ,metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:44:25.199064Z","iopub.execute_input":"2025-07-02T12:44:25.199600Z","iopub.status.idle":"2025-07-02T12:44:27.693470Z","shell.execute_reply.started":"2025-07-02T12:44:25.199578Z","shell.execute_reply":"2025-07-02T12:44:27.692929Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:44:43.480833Z","iopub.execute_input":"2025-07-02T12:44:43.481103Z","iopub.status.idle":"2025-07-02T12:44:43.506733Z","shell.execute_reply.started":"2025-07-02T12:44:43.481083Z","shell.execute_reply":"2025-07-02T12:44:43.506165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop= EarlyStopping(monitor='val_loss', patience=13, restore_best_weights=True)\ncheckpoint = ModelCheckpoint('best_model.h5',monitor='val_accuracy',save_best_only=True,mode='max')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:45:08.791668Z","iopub.execute_input":"2025-07-02T12:45:08.792105Z","iopub.status.idle":"2025-07-02T12:45:08.795914Z","shell.execute_reply.started":"2025-07-02T12:45:08.792083Z","shell.execute_reply":"2025-07-02T12:45:08.795235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_generator,\n    epochs=70,\n    validation_data=val_generator,\n    callbacks=[early_stop, checkpoint]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T12:45:20.095810Z","iopub.execute_input":"2025-07-02T12:45:20.096054Z","iopub.status.idle":"2025-07-02T13:28:43.515679Z","shell.execute_reply.started":"2025-07-02T12:45:20.096038Z","shell.execute_reply":"2025-07-02T13:28:43.515040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Val Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training vs Validation Accuracy')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:29:21.361642Z","iopub.execute_input":"2025-07-02T13:29:21.362270Z","iopub.status.idle":"2025-07-02T13:29:21.554401Z","shell.execute_reply.started":"2025-07-02T13:29:21.362240Z","shell.execute_reply":"2025-07-02T13:29:21.553635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['loss'], label = 'Train Loss')\nplt.plot(history.history['val_loss'], label = 'Validation Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:29:35.229723Z","iopub.execute_input":"2025-07-02T13:29:35.230241Z","iopub.status.idle":"2025-07-02T13:29:35.372982Z","shell.execute_reply.started":"2025-07-02T13:29:35.230218Z","shell.execute_reply":"2025-07-02T13:29:35.372306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, acc = model.evaluate(test_generator)\nprint(f\"Test accuracy: {acc:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:29:57.887879Z","iopub.execute_input":"2025-07-02T13:29:57.888168Z","iopub.status.idle":"2025-07-02T13:30:03.983915Z","shell.execute_reply.started":"2025-07-02T13:29:57.888147Z","shell.execute_reply":"2025-07-02T13:30:03.983265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(test_generator)\npredicted_classes = np.argmax(predictions, axis=1)\ntrue_classes = test_generator.classes\nclass_labels = list(test_generator.class_indices.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:30:15.750991Z","iopub.execute_input":"2025-07-02T13:30:15.751263Z","iopub.status.idle":"2025-07-02T13:30:20.893036Z","shell.execute_reply.started":"2025-07-02T13:30:15.751243Z","shell.execute_reply":"2025-07-02T13:30:20.892406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"report = classification_report(true_classes, predicted_classes, target_names=class_labels)\nprint(\"Classification Report:\\n\", report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:30:31.870136Z","iopub.execute_input":"2025-07-02T13:30:31.870641Z","iopub.status.idle":"2025-07-02T13:30:31.882612Z","shell.execute_reply.started":"2025-07-02T13:30:31.870615Z","shell.execute_reply":"2025-07-02T13:30:31.881905Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(true_classes, predicted_classes)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_labels,\n            yticklabels=class_labels)\n\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:31:00.000474Z","iopub.execute_input":"2025-07-02T13:31:00.001165Z","iopub.status.idle":"2025-07-02T13:31:00.297497Z","shell.execute_reply.started":"2025-07-02T13:31:00.001141Z","shell.execute_reply":"2025-07-02T13:31:00.296716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get class label mappings\nclass_indices = train_generator.class_indices\ninv_class_indices = {v: k for k, v in class_indices.items()}\n\n# Get a batch of test data\ntest_images, test_labels = next(test_generator)\n\n# Predict\npreds = model.predict(test_images)\n\nfor i in range(5):  # Show first 5 predictions\n    plt.imshow(test_images[i])\n    pred_class = inv_class_indices[np.argmax(preds[i])]\n    true_class = inv_class_indices[np.argmax(test_labels[i])]\n    plt.title(f\"Pred: {pred_class}, True: {true_class}\")\n    plt.axis('off')\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T13:41:24.511938Z","iopub.execute_input":"2025-07-02T13:41:24.512696Z","iopub.status.idle":"2025-07-02T13:41:25.581914Z","shell.execute_reply.started":"2025-07-02T13:41:24.512671Z","shell.execute_reply":"2025-07-02T13:41:25.581141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Related Works**\n# Paper(1): Mouth and oral disease classifcation using InceptionResNetV2 method\n\nThis study presents a deep learning approach for diagnosing **seven oral diseases** using the **InceptionResNetV2** architecture and a custom-built dataset.\n\n---\n\n##  Objective\n\nDevelop an accurate and automated classifier for:\n- Canker Sores (CaS)\n- Cold Sores (CoS)\n- Gingivostomatitis (Gum)\n- Mouth Cancer (MC)\n- Oral Cancer (OC)\n- Oral Lichen Planus (OLP)\n- Oral Thrush (OT)\n\n---\n\n##  MOD Dataset\n\n- 517 raw oral disease images collected from clinics and online sources\n- Expert annotated and categorized into 7 classes\n- Augmentation expanded the dataset to ~5143 images\n- Split: 60% Train, 20% Validation, 20% Test\n\n---\n\n##  Methodology\n\n**Model Used:** InceptionResNetV2 with transfer learning  \n**Training Setup:**\n- Input Size: 224×224\n- Batch Size: 4\n- Epochs: 50\n- Optimizer: Adam (LR=0.0001)\n- Loss: Categorical Crossentropy\n- Early stopping on `val_loss`\n- Checkpoint on best `val_accuracy`\n\n**Preprocessing Includes:**\n- Rescaling\n- Rotation, shear, zoom\n- Horizontal flip\n- Brightness and channel shifts\n\n---\n\n##  Results\n\n**With Data Augmentation:**\n- **Accuracy**: 99.51%\n- **F1-scores**: >98% on all classes\n- **AUC Scores**: ≥ 99% per class\n\n**Without Augmentation:**\n- Accuracy dropped to 74.07%\n- Demonstrates augmentation's importance\n\n---\n\n##  Comparison to Previous Work\n\n| Study | Model | Accuracy |\n|-------|-------|----------|\n| Park et al. | CNN | 95.00% |\n| Gomes et al. | CNN | 95.09% |\n| Singha et al. | ResNet50 + DenseNet201 | 92.00% |\n| Jaiswal et al. | EfficientNetB0 | 93.20% |\n| **Proposed** | InceptionResNetV2 | **99.51%** |\n\n---\n\n## Conclusion\n\n- InceptionResNetV2 shows state-of-the-art results in oral disease classification.\n- The MOD dataset addresses a critical gap in available medical datasets.\n- The model is promising for real-world deployment in diagnostic systems.\n\n\n\n# Paper2: Use of Artificial Intelligence in the Classification of Elementary Oral Lesions from Clinical Images\n\n\n## Objective\n\nDevelop a deep learning model to classify clinical oral lesion images into **six elementary lesion types**:\n\n- Papule/Nodule  \n- Macule/Spot  \n- Vesicle/Bullous  \n- Erosion  \n- Ulcer  \n- Plaque\n\nThe goal is to support early diagnosis and aid non-specialist practitioners.\n\n---\n\n##  Dataset\n\n- **Total Images:** 5069 anonymized clinical oral lesion images  \n- **Source:** Hospital de Clínicas de Porto Alegre and UFRGS Dentistry School (Brazil)  \n- **Labeling:** Manually annotated by specialists  \n- **Split:**  \n  - 70% Training (3550 images)  \n  - 30% Testing (1519 images)  \n- **Variability:** Includes both high- and low-quality images from varied devices (SLRs, smartphones)\n\n---\n\n## Methodology\n\n### Models Tested:\n\n- VGG16  \n- ResNet-50  \n- Xception  \n- **InceptionV3**  *(Best performance)*\n\nAll models were pretrained on ImageNet and fine-tuned for 6-class classification.\n\n### Data Augmentation:\n\n| Class           | Original | Augmentation Factor | Final Count |\n|------------------|----------|----------------------|-------------|\n| Plaque           | 1618     | 1×                   | 1618        |\n| Erosion          | 159      | 10×                  | 1590        |\n| Vesicle/Bullous  | 308      | 5×                   | 1540        |\n| Ulcer            | 1506     | 1×                   | 1506        |\n| Papule/Nodule    | 1417     | 1×                   | 1417        |\n| Macule/Spot      | 61       | 30×                  | 1830        |\n\n---\n\n##  Training Setup\n\n- **Input Size:** 299×299 (InceptionV3)\n- **Optimizer:** Adam  \n- **Learning Rate:** 1e-6  \n- **Loss Function:** Categorical Crossentropy  \n- **Epochs:** 50  \n- **Frozen Layers:** All except the last few for fine-tuning\n\n---\n\n##  Results\n\n| Class           | Accuracy | Precision | Sensitivity | Specificity | F1-Score |\n|------------------|----------|-----------|-------------|-------------|----------|\n| Ulcer            | 91.30%   | 74.73%    | 71.71%      | 95.19%      | 73.19%   |\n| Papule/Nodule    | 92.14%   | 74.52%    | 79.79%      | 94.58%      | 77.07%   |\n| Macule/Spot      | 98.99%   | 100.00%   | 94.00%      | 100.00%     | 96.90%   |\n| Plaque           | 92.14%   | 71.90%    | 87.00%      | 93.17%      | 78.73%   |\n| Erosion          | 96.32%   | 98.75%    | 79.00%      | 99.79%      | 87.77%   |\n| Vesicle/Bullous  | 99.66%   | 98.03%    | 100.00%     | 99.59%      | 99.01%   |\n\n**Overall Performance:**\n- Accuracy: **95.09%**  \n- F1-Score: **85.44%**  \n- Precision: **86.32%**  \n- Sensitivity: **85.25%**  \n- Specificity: **97.05%**\n\n---\n\n##  Discussion\n\n- Strong performance even with varied image quality  \n- Highest accuracy observed for Macule/Spot and Vesicle/Bullous  \n- Most confusion occurred between Ulcers and Papules due to visual similarity  \n- Model generalization supported by augmentation\n\n---\n\n##  Conclusion\n\nThe InceptionV3-based model achieved excellent performance in classifying oral lesion types, supporting the feasibility of AI in clinical decision support for dentistry and oral pathology.\n\n","metadata":{}}]}